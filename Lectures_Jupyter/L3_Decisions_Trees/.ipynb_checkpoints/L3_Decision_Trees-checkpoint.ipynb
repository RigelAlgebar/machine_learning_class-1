{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<font size='7' style=\"color:#0D47A1\">  <b>DECISION TREES</b> </font>\n",
    "</center>\n",
    "\n",
    "<hr style= \"height:3px;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' style=\"color:#00A6D6\">  <b>Introduction. What are they?</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are predictive models that choose distinguishing observations, or features, about a set of data to predict each data element's target value, also known as the data element's class. \n",
    "\n",
    "To further explain, each feature is represented in the internal nodes and branches of the tree. At each node, we see a split of the data set on that feature, either leading to another internal node with a another distinguishing observation or to a specific class, called a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "Image(filename='./Decision-Trees.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Types of Decision Trees</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Classification Trees**\n",
    "    - An analysis where the target data, or classes, are discretized\n",
    "- **Regression Trees**\n",
    "    - An analysis where the classes are continuous, i.e. they take on real numbers.\n",
    "        - *This is not to say they have infinite domain*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Ensembles</b></font>\n",
    "\n",
    "Decision trees are commonly used in ensembles to better predict the classes within a set of data. There are many techniques to create these ensembles, some common ones are:\n",
    "\n",
    "- Boosted Trees\n",
    "    - incrementally building an ensemble of trees by applying boosted methods\n",
    "- Bagged Trees\n",
    "    - Building multiple trees by resampling the data each time and deciding on a class by a consensus prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following program you can change the depth value to visualize how the classifier splits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%matplotlib inline \n",
    "X, y = make_blobs(centers=[[0, 0], [1, 1]], n_samples=50)\n",
    "\n",
    "def plot_forest(max_depth=1):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    h = 0.02\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    if max_depth != 0:\n",
    "        forest = RandomForestClassifier(n_estimators=1, max_depth=max_depth,\n",
    "                                        random_state=1).fit(X, y)\n",
    "        Z = forest.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, alpha=.4)\n",
    "        ax.set_title(\"max_depth = %d\" % max_depth)\n",
    "    else:\n",
    "        ax.set_title(\"data set\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=np.array(['b', 'r'])[y], s=60)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_forest_interactive():\n",
    "    from ipywidgets import interactive, IntSlider\n",
    "    slider = IntSlider(min=0, max=8, step=1, value=0)\n",
    "    return interactive(plot_forest, max_depth=slider)\n",
    "plot_forest_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above program there is a parameter called n_estimators. If you change the value of that number, the algorithm will consider several trees to make their decision, this are called random forest and you can find information about them here:https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Strengths/Weaknesses of a Decision Tree</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Some Advantages</b></font>\n",
    "\n",
    " - Simple to understand\n",
    " - Can Handle both numerical and categorical data (We will see this soon)\n",
    " - Relatively little Data preparation needed\n",
    " - The mechanism for the model can be easily extracted and understood\n",
    " - Robust against co-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Some Disadvantages</b></font>\n",
    "\n",
    "- Accuracy\n",
    "- Not very robust in general\n",
    "- Overfitting\n",
    "- Locality of Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>How to Build a Decision Tree?</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>ID3</b></font>\n",
    "\n",
    "This method uses a maximization of Information gain or a minimization of entropy to determine how to split the tree at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Definitions</b></font>\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "$$\n",
    "H(X) = \\sum_{y \\in Y} -p(y) log_2 p(y)\n",
    "$$\n",
    "\n",
    "where\n",
    "- X is a data set\n",
    "- Y is the subset of classes in X\n",
    "- p(y) is the proportion of the class y in the subset Y\n",
    "\n",
    "**Information Gain**\n",
    "\n",
    "$$\n",
    "IG(A, X) = H(X) - \\sum_{s \\in S} p(s)H(s)\n",
    "$$\n",
    "\n",
    "where\n",
    "- A is the attribute we are splitting the data on\n",
    "- S is the set of subsets created when the data was split, i.e. \n",
    "$$ \n",
    "X = \\cup_{s \\in S} s\n",
    "$$\n",
    "- p(s) is the proportion of the amount of elements in s to total amount of elements in X\n",
    "\n",
    "*In ID3, we choose to split data on either the largest energy gain or the smallest entropy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>ID3 Pseudocode</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./pseudo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Summary</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [1].\n",
    "\n",
    "Classification Tree are used when you have Discrete output.\n",
    "\n",
    "It's a Supervised Learning algorithm (ID3, C4.5).\n",
    "\n",
    "When to use it:\n",
    "    \n",
    "    when we have a finite number of classification categories\n",
    "    the data can be represented as vectors\n",
    "    you want to understand how the classifier makes its choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Example</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example from Brian Pardo's course at NU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./problem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Origin = [0, 0, 0, 1, 0]\n",
    "Manu = [0, 2, 2, 1, 0]\n",
    "Color = [0, 1, 0, 2, 3]\n",
    "Year = [1, 0, 2, 1, 1]\n",
    "Type = [0, 1, 0, 0, 0]\n",
    "target = [1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(a):\n",
    "    return {c: (a==c).nonzero()[0] for c in np.unique(a)}\n",
    "#for splitting a set\n",
    "# maps each unique value to its indicies in an array\n",
    "# nonzero: return the indices of the elements that are non-zero.\n",
    "\n",
    "print(partition(Origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(s):\n",
    "    res = 0\n",
    "    val, counts = np.unique(s, return_counts=True)\n",
    "    freqs = counts.astype('float')/len(s)\n",
    "    \n",
    "    for p in freqs:\n",
    "        if p != 0.0:\n",
    "            res -= p * np.log2(p)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate([Origin, Manu, Color, Year, Type]):\n",
    "    print('feature ',i,' has entropy: ', entropy(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose Origin as the root of the Tree. All cars with origin USA have a classification of 0. We calculate the entropy of the remaining unused attributes on the data that has Origin=Japan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ManuJapan = [0, 2, 2,  0]\n",
    "ColorJapan = [0, 1, 0,  3]\n",
    "YearJapan = [1, 0, 2,  1]\n",
    "TypeJapan = [0, 1, 0,  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate([ManuJapan, ColorJapan, YearJapan, TypeJapan]):\n",
    "    print('feature ',i+1,' has entropy: ', entropy(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can select feature 4=Type. After asking two questions, our Tree describes the concept \"Japanese Economic Car\" on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Homework</b> </font>\n",
    "\n",
    "The following code imports the Wine data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.renderers.enable('notebook')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Characteristics.\n",
    "\n",
    "Number of Instances: 178 (50 in each of three classes)\n",
    "\n",
    "Number of Attributes: 13 numeric, predictive attributes and the class\n",
    "\n",
    "\n",
    " \t\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Attribute Information:')\n",
    "data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classes:')\n",
    "np.unique(data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the notebook \"First Machine Learning Notebook\" and repeat the analysis with the Wine dataset. Note that now we have more attibutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We move it to panda \n",
    "index = [i for i in range(len(data['data']))]\n",
    "#wine_Data has the data and classes has the labels\n",
    "wine_Data = pd.DataFrame(data = np.float_(data['data']), index = index, columns = data['feature_names'])\n",
    "classes=np.array(np.int_(data['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_Data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the class as a feature to do visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "wineWithLabel = wine_Data.copy(deep=True)\n",
    "wineWithLabel['class']=classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.pairplot(wineWithLabel,hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an interactive visualization of the data. (select regions on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = alt.selection_interval()\n",
    "base = alt.Chart(wineWithLabel).mark_point().encode(\n",
    "    y = 'flavanoids',\n",
    "    color = alt.condition(interval,'class', alt.value('lightgray')),\n",
    "    tooltip=\"class\"\n",
    ").add_selection(interval)\n",
    "base.encode(x='total_phenols')|base.encode(x=\"color_intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are only 3 classes, and we can see that there are no missing values on the 'alcohol' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(wineWithLabel['class'].unique())==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(wineWithLabel.loc[(wineWithLabel['alcohol'].isnull())])==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with the analysis and create a classifier, then evaluate the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>References</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1] http://www.cs.northwestern.edu/~pardo/courses/eecs349/index.php\n",
    "\n",
    "[2] https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Decision_tree\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Random_forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
