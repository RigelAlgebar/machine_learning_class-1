{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of Bayesian Interpretation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Interpretation leading to Inference\n",
    "\n",
    "When information is not sufficient in of itself for the truth to be obvious, we depend on using the available information to **infer** truth. The methods and tools used to do so reside with probability theory.\n",
    "\n",
    "*How often do we find ourselves with insufficient information?*\n",
    "\n",
    "This connection between probability and inference makes probability theory and its interpretation paramount to how we study and understand the world around us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frequentist Interpretation\n",
    "\n",
    "This school of thought will seem most familiar and, in my opinion, easy to grasp. The frequentist interpretation centers around the measurement of the number of occurences of a random event in a sufficiently large number of identical and independent trials.\n",
    "\n",
    "The appeal here can be easily seen. It seems the most scientific and objective form to develop and interpret probabilities. For example, I tell you that the probability of flipping a coin and getting heads is $ 1/2 $ and you would assume that, after flipping a coin hundreds of times or more, approximately half would be heads. You'd be right of course, it just makes sense. The frequentist approach by counting these random events in performing repeated identical and independent trials seems to result in the most onjective from of measurment and assignment of probabilities. Although, there are disadvantages to this outlook. How would a frequentist answer the following?\n",
    "\n",
    "*What does it mean to be random?*\n",
    "\n",
    "*If trials were conducted in a truly (or sufficiently) identical way, wouldn't one expect the same result?*\n",
    "\n",
    "*If the probabilistic results are to be appilicable and operational, how many trials is enough?*\n",
    "\n",
    "These answers are not simple and, in regards to some, could be subjective. Therefore, these questions are controversial and lead to uncertainty in the frequentist interpretation. \n",
    "\n",
    "Questions aside however, this view works perfectly well, except when trials cannot be repeated. When there is a lack of a set of trials, and a large set I might add, the frequentist approach falls apart. Such problems include asking the question, what is the probability of finding life on Venus or Europa. This cannot be repeated multiple times in a trial setting. Therefore, there is a need for an alternative interpretation. Enter Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian Interpretation\n",
    "\n",
    "Bayesian interpretation, actually founded by Laplace and Bernoulli, takes its meaning from an individual deciding how much confidence, or degree of belief, they have in the truth of a proposition. By moving to this realm where, instead of relying on repeatable events, we reason the probability of a single event, the questions posed for the frequentist interpretation do not apply. Additionally, the scope of problems that can be addressed is significantly increased.\n",
    "\n",
    "Although, this leads to the discussion of how objective can this interpretation be. On the one hand, one could argue that two individuals, given the same information, could conclude on two separate and unique probabilities. On the other hand, one could argue that, under the same circumstances, two rational and logical people only drawing from identical information could only conclude the same things and that only new information could change their minds.\n",
    "\n",
    "While judging probabilities of events may always have an objective nature, the way we process information can be objective and therefore supress the subjective judgement. It follows to reason that the more information we attain and process, the greater we enhance the objective portion of our probabilities and the more useful they become. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok, but how does it work? Bayes Rule/Theorem (Or is it Laplace's?)\n",
    "\n",
    "Say we are about to look for new data on some parameter $\\theta$ and other data $x$. We have some prior knowledge about the parameter in the form of the probability distribution $ q(\\theta) $ and some prior knowledge about how $x$ and $\\theta$ relate in the form of $q(x | \\theta)$, therefore we have the joint probability distribution, \n",
    "\n",
    "$$\n",
    "q(\\theta, x) = q(\\theta)q(x | \\theta)\n",
    "$$\n",
    "\n",
    "Now we measure and find our our new values, $x'$. We wish to update our prior beliefs to a posterior belief represented as $p(\\theta, x)$. Therefore, our new beliefs must satisfy:\n",
    "\n",
    "$$\n",
    "p(x) = \\int d\\theta p(\\theta, x) = \\delta(x - x')\n",
    "$$\n",
    "\n",
    "*A quick word on Notation: all functions q are prior probability distributions, all functions p are posterior probability distributions, x' refers to the new data collected, and x refers to a random variable.*\n",
    "\n",
    "This is not sufficient for finding the new joint distribution. To solve this issue, we imploy **the Prinicipal of Minimal Updating**. It states that the new beliefs only need be updated as far as the new information requires. Therefore,\n",
    "\n",
    "$$\n",
    "q(x) \\Rightarrow p(x) = \\delta(x - x')\n",
    "$$\n",
    "\n",
    "is enough and so $q(\\theta | x') = p(\\theta | x')$.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "p(\\theta, x) = p(x)p(\\theta | x') = \\delta(x - x')q(\\theta | x')\n",
    "$$\n",
    "\n",
    "To obtain $p(\\theta)$,\n",
    "\n",
    "$$\n",
    "p(\\theta) = \\int dxp(\\theta, x) = \\int dx \\delta(x - x')q(\\theta | x')\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow\n",
    "$$\n",
    "$$\n",
    "p(\\theta = q(\\theta | x')\n",
    "$$\n",
    "\n",
    "This is Bayes' Rule, that *the posterior probability equals the prior conditional probability of $\\theta$ given $x'$*.\n",
    "\n",
    "Written another way,\n",
    "\n",
    "$$\n",
    "p(\\theta) = q(\\theta)\\frac{q(x'|\\theta)}{q(x')}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Danger\n",
    "\n",
    "Deciding the prior in this method is a difficult task and can be screwed up if done incorrectly. There are pitfalls to aviod and steps that must be carefully taken.\n",
    "\n",
    "##### Erroneous Absolutes\n",
    "\n",
    "One pitfall is assigning an absolute prior, meaning say absolute certainty or $q(\\theta | B) = 1$. In this case, $\\theta B = B$ and so $q(x | \\theta B) = q(x | B)$. Therefore,\n",
    "\n",
    "$$\n",
    "p(\\theta | B) = q(\\theta | B)\\frac{q(x | \\theta B)}{q(x | B)} = 1\n",
    "$$\n",
    "\n",
    "The same thing can be shown for a case of impossibility. When a prior like this is chosen, we are saying that there is no data, new or otherwise, that could convince us different from our prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Cool Book Peter Referred to me by Ariel Caticha at the University of Albany, still reading it, its pretty good\n",
    "https://www.albany.edu/physics/ACaticha-EIFP-book.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
